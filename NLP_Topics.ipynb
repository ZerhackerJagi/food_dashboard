{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Ansatz, um Posts zu Differenzieren\n",
    "\n",
    "Folgend werden Posts geclustert und unterschiedlichen Kategorien zugewiesen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Variablen\n",
    "API_KEY_SPOONACULAR = \"146fe79a630b4afb8306a2985bedc64c\"\n",
    "API_KEY_SPOONACULAR = \"6e0a91c81983423792b327bc4b78bb1d\"\n",
    "\n",
    "API_KEY_REDDIT = \"KAJ2xGzCOlH3hyTRADNdSBDuTBORvw\"\n",
    "API_CLIENTID_REDDIT = \"tSemvcUBRFCTYQ\"\n",
    "\n",
    "USERNAME_REDDIT = \"Short-Arrival7632\"\n",
    "PW_REDDIT = \"cookies123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KONFIGURATION\n",
    "\n",
    "LIMIT_REDDIT_POSTS = 100\n",
    "LIMIT_SPOONACULAR_RECIPES = 100\n",
    "REDDIT_SAVE_INTERVALL = 10 # Minuten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auf Windows ausführen (einmalig)\n",
    "# !pip install praw\n",
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as pdt\n",
    "import mysql.connector\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_connection = mysql.connector.connect(\n",
    "  host= \"wp.jagi.wtf\",\n",
    "  user= \"redditu\",\n",
    "  password= \"redditMaster\",\n",
    "  database=\"reddit\"\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = db_connection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data from Subreddit DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(\"SELECT * FROM reddit\", db_connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP STARTS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['hour_created', 'time_created', 'day_created', 'author',\n",
    "       'ups', 'downs', 'num_comments', 'text', 'thumbnail', 'url', 'curr_time'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auf Windows ausführen (einmalig)\n",
    "# !pip install tmtoolkit\n",
    "# !pip install nltk\n",
    "# !pip install pyLDAvis\n",
    "# !pip install\n",
    "# !pip install tqdm\n",
    "# !pip install spacy==2.3.5\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "# one time run to make this notebook work\n",
    "# import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# maths\n",
    "import numpy as np\n",
    "\n",
    "# regular expressions to manipulate text\n",
    "import re\n",
    "import string\n",
    "\n",
    "# NLP\n",
    "from tmtoolkit.preprocess import TMPreproc\n",
    "from tmtoolkit.corpus import Corpus\n",
    "# from tmtoolkit.defaults import language\n",
    "from tmtoolkit.topicmod.tm_lda import evaluate_topic_models\n",
    "from tmtoolkit.topicmod.evaluate import results_by_parameter\n",
    "from tmtoolkit.topicmod.visualize import plot_eval_results\n",
    "from tmtoolkit.topicmod.model_io import save_ldamodel_summary_to_excel\n",
    "from tmtoolkit.topicmod.visualize import parameters_for_ldavis\n",
    "from tmtoolkit.topicmod.model_io import save_ldamodel_to_pickle\n",
    "from tmtoolkit.topicmod.model_io import ldamodel_top_topic_words\n",
    "from tmtoolkit.topicmod.model_stats import marginal_topic_distrib\n",
    "from tmtoolkit.bow.bow_stats import doc_lengths\n",
    "from tmtoolkit.topicmod.model_stats import generate_topic_labels_from_top_words\n",
    "\n",
    "import en_core_web_sm\n",
    "\n",
    "import pyLDAvis\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# progress bars\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## simple preprocessing\n",
    "\n",
    "## Notiz an mich selbst 03.04.2020\n",
    "## Nachschauen, wie regular expressions jetzt funktionieren\n",
    "\n",
    "print('amount of data rows before: {}'.format(len(df)))\n",
    "\n",
    "\n",
    "for idx in tqdm(df.index):\n",
    "    text = df.loc[idx,'title']\n",
    "#     # remove url's\n",
    "#     # from https://docs.microsoft.com/en-us/previous-versions/msp-n-p/ff650303(v=pandp.10)?redirectedfrom=MSDN\n",
    "#     text = re.sub(\"^(ht|f)tp(s?)\\:\\/\\/[0-9a-zA-Z]([-.\\w]*[0-9a-zA-Z])*(:(0-9)*)*(\\/?)([a-zA-Z0-9\\-\\.\\?\\,\\'\\/\\\\\\+&amp;%\\$#_]*)?$\",                   '', text)\n",
    "#     # mail adresses\n",
    "#     text = re.sub('\\S+@\\S+', '', text)\n",
    "#     # web pages\n",
    "#     text = re.sub('\\S+.com', '', text)\n",
    "#     # digits\n",
    "#     text = re.sub('\\w*\\d\\w*', '', text)\n",
    "#     # non-sensical text\n",
    "#     text = re.sub('\\n', ' ', text)\n",
    "#     text = re.sub('\\r', ' ', text)\n",
    "#     text = re.sub('\\\\xa0', ' ', text)\n",
    "    # german letters\n",
    "    text = re.sub('ö','oe',text)\n",
    "    text = re.sub('Ö','Oe',text)\n",
    "    text = re.sub('ü','ue',text)\n",
    "    text = re.sub('Ü','Ue',text)\n",
    "    text = re.sub('ä','ae',text)\n",
    "    text = re.sub('Ä','Ae',text)\n",
    "    text = re.sub('ß','ss',text)\n",
    "    # removing extra white spaces\n",
    "    text = \" \".join(text.split())\n",
    "    df.loc[idx,'title'] = text\n",
    "    \n",
    "df = df.dropna()\n",
    "print('amount of data rows after: {}'.format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## globals\n",
    "\n",
    "parent_path = os.path.dirname(os.getcwd())\n",
    "language = 'english'\n",
    "\n",
    "# suppress the \"INFO\" messages and warnings from lda\n",
    "logger = logging.getLogger('lda')\n",
    "logger.addHandler(logging.NullHandler())\n",
    "logger.propagate = False\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "stop = ['homemade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## formatting for tmtoolkit\n",
    "d = df.set_index('id')['title'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmtoolkit preprocessing\n",
    "## multiple preprocessing steps to be computed and compared against each other\n",
    "\n",
    "corpus = Corpus(docs=d)\n",
    "print(corpus)\n",
    "#corpus = corpus.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp0 = TMPreproc(corpus, language='en')\n",
    "del corpus\n",
    "\n",
    "old_vocab = pp0.vocabulary\n",
    "print('vocabulary size before processing: {}'.format(pp0.vocabulary_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## general simple preprocessing\n",
    "\n",
    "pp0.pos_tag().lemmatize().tokens_to_lowercase().remove_special_chars_in_tokens()\n",
    "\n",
    "general_vocab = pp0.vocabulary\n",
    "\n",
    "print('vocabulary size after processing: {}'.format(pp0.vocabulary_size))\n",
    "print('vocabulary, that was cut during this step:')\n",
    "print(set(old_vocab) - set(general_vocab))\n",
    "del old_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## specific preprocessing\n",
    "\n",
    "pp_aggressive = pp_nouns = pp_noadditionalstopwords = pp_staticstopwords = pp_dynamicstopwords = pp0.copy()\n",
    "doc_labels = np.array(pp0.doc_labels)\n",
    "del pp0\n",
    "\n",
    "pp_dynamicstopwords.clean_tokens(remove_numbers=True, remove_shorter_than=2) \\\n",
    "    .remove_common_tokens(df_threshold=0.9) \\\n",
    "    .remove_uncommon_tokens(df_threshold=0.1)\n",
    "\n",
    "pp_staticstopwords.add_stopwords(stop) \\\n",
    "    .clean_tokens(remove_numbers=True, remove_shorter_than=2) \\\n",
    "    .remove_uncommon_tokens(2, absolute=True)\n",
    "\n",
    "pp_noadditionalstopwords.clean_tokens(remove_numbers=True, remove_shorter_than=2) \\\n",
    "    .remove_common_tokens(df_threshold=0.85) \\\n",
    "    .remove_uncommon_tokens(df_threshold=0.05)\n",
    "\n",
    "pp_nouns.filter_for_pos('N') \\\n",
    "    .add_stopwords(stop) \\\n",
    "    .clean_tokens(remove_numbers=True, remove_shorter_than=3) \\\n",
    "    .remove_common_tokens(df_threshold=0.85) \\\n",
    "    .remove_uncommon_tokens(df_threshold=0.05)\n",
    "\n",
    "pp_aggressive.filter_for_pos('N') \\\n",
    "    .add_stopwords(stop) \\\n",
    "    .clean_tokens(remove_numbers=True, remove_shorter_than=3) \\\n",
    "    .remove_common_tokens(df_threshold=0.9) \\\n",
    "    .remove_uncommon_tokens(df_threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab\n",
    "vocab_dynamicstopwords = np.array(pp_dynamicstopwords.vocabulary)\n",
    "vocab_staticstopwords = np.array(pp_staticstopwords.vocabulary)\n",
    "vocab_noadditionalstopwords = np.array(pp_noadditionalstopwords.vocabulary)\n",
    "vocab_nouns = np.array(pp_nouns.vocabulary)\n",
    "vocab_aggressive = np.array(pp_aggressive.vocabulary)\n",
    "\n",
    "#dtm\n",
    "dtm_dynamicstopwords = pp_dynamicstopwords.dtm\n",
    "dtm_staticstopwords = pp_staticstopwords.dtm\n",
    "dtm_noadditionalstopwords = pp_noadditionalstopwords.dtm\n",
    "dtm_nouns = pp_nouns.dtm\n",
    "dtm_aggressive = pp_aggressive.dtm\n",
    "del pp_dynamicstopwords, pp_staticstopwords, pp_noadditionalstopwords, pp_nouns, pp_aggressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data and hyperparameters to be computed\n",
    "\n",
    "dtms = {\n",
    "    'model_dynamicstopwords': dtm_dynamicstopwords,\n",
    "    'model_staticstopwords': dtm_staticstopwords,\n",
    "    'model_noadditionalstopwords': dtm_noadditionalstopwords,\n",
    "    'model_nouns': dtm_nouns,\n",
    "    'model_aggressive': dtm_aggressive\n",
    "}\n",
    "\n",
    "var_params = [{'n_topics': k, 'alpha': 1/k, 'eta': 0.028*k,'n_iter': 200*k} for k in range(5, 8, 10, 12, 15, 18, 20, 25)]\n",
    "\n",
    "const_params = {\n",
    "    #'n_iter': 500#,\n",
    "    'random_state': 69  # to make results reproducible\n",
    "}\n",
    "\n",
    "var_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute all topic models and plot evaluation results\n",
    "\n",
    "for dtm in tqdm(dtms.keys()):\n",
    "    dtm_name = dtm\n",
    "    dtm = dtms[dtm]\n",
    "\n",
    "    eval_results = evaluate_topic_models(dtm,\n",
    "                                        varying_parameters=var_params,\n",
    "                                        constant_parameters=const_params,\n",
    "                                        return_models=False)\n",
    "\n",
    "    #print('current dtm: {}'.format(dtm))\n",
    "    eval_results_by_topics = results_by_parameter(eval_results, 'n_topics')\n",
    "    plot_eval_results(eval_results_by_topics, xaxislabel=dtm_name + ' | n_topics');\n",
    "\n",
    "    eval_results_by_topics = results_by_parameter(eval_results, 'n_iter')\n",
    "    plot_eval_results(eval_results_by_topics, xaxislabel=dtm_name + ' | n_iter');\n",
    "\n",
    "    eval_results_by_topics = results_by_parameter(eval_results, 'alpha')\n",
    "    plot_eval_results(eval_results_by_topics, xaxislabel=dtm_name + ' | alpha');\n",
    "\n",
    "    eval_results_by_topics = results_by_parameter(eval_results, 'eta')\n",
    "    plot_eval_results(eval_results_by_topics, xaxislabel=dtm_name + ' | eta');\n",
    "\n",
    "    print('{} done - waiting for plots'.format(dtm_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## best parameter sets for our different dtms \n",
    "'''\n",
    "\n",
    "--- notes ---\n",
    "alpha: per document topic distribution (higher)\n",
    "eta: per topic word distribution (lower)\n",
    "\n",
    "'''\n",
    "# select dtm for following steps \n",
    "dtm = dtms['model_staticstopwords']\n",
    "vocab = vocab_staticstopwords\n",
    "print('selected model removed the following vocabulary:')\n",
    "print(set(general_vocab) - set(vocab))\n",
    "#del general_vocab, dtms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
